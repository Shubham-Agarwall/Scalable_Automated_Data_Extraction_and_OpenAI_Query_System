{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d0e6422-57ba-4380-95f8-952a310ed976",
   "metadata": {},
   "source": [
    "# Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d1efd-597c-4a64-936e-b8cbb6036552",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1717d2ee-55ef-49ec-9da1-746fa247f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-cloud-storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25796b6-8b43-46d4-842e-87c524a7f703",
   "metadata": {},
   "source": [
    "# Inporting and using required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a8b93-1ec1-4a2a-b3bb-a88d02ca500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "from google.cloud import storage\n",
    "import subprocess\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447f1438-c9da-40e3-a1c6-7a863c29cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to snapshot the gaia-benchmark/GAIA dataset to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de376b61-68ee-4ab1-bf6a-a1388576511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Function to download the dataset to a desired local path\n",
    "def download_dataset_to_local(repo_id, download_path):\n",
    "   \n",
    "    print(f\"Downloading dataset from Hugging Face to {download_path}...\")\n",
    "    local_dir = snapshot_download(repo_id, repo_type=\"dataset\", local_dir=download_path, revision='main')\n",
    "    print(f\"Dataset downloaded to {local_dir}\")\n",
    "    return local_dir\n",
    "\n",
    "\n",
    "HUGGING_FACE_REPO_ID = \"gaia-benchmark/GAIA\" \n",
    "DESIRED_LOCAL_PATH = \"/Users/shubhamagarwal/Documents/Northeastern/Big_Data_Assignment_2/gaia_dataset\"  \n",
    "\n",
    "\n",
    "local_dataset_path = download_dataset_to_local(HUGGING_FACE_REPO_ID, DESIRED_LOCAL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f8d85-0359-4a65-8d58-5fcf1fd51fee",
   "metadata": {},
   "source": [
    "# Fuction to scan the dataset for malware clamscan library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c2cf0-d61a-4892-ac60-ee1ba7e4cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Function to scan a file for malware using ClamAV\n",
    "def scan_for_malware(file_path):\n",
    "    \"\"\"Scans a file for malware using ClamAV.\"\"\"\n",
    "    result = subprocess.run(['clamscan', file_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    output = result.stdout.decode()\n",
    "\n",
    "    if \"Infected files: 0\" in output:\n",
    "        return True \n",
    "    else:\n",
    "        return False  \n",
    "\n",
    "# Function to scan all files in the local dataset directory\n",
    "def scan_local_dataset(local_dir):\n",
    "\n",
    "    total_files = 0\n",
    "    clean_files = 0\n",
    "    infected_files = 0\n",
    "    total_dirs = 0\n",
    "    \n",
    "    print(\"Scanning files for malware...\")\n",
    "    for root, dirs, files in os.walk(local_dir):\n",
    "        total_dirs += 1\n",
    "        for file in files:\n",
    "            total_files += 1\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            # Scan each file and record the result\n",
    "            if scan_for_malware(file_path):\n",
    "                clean_files += 1\n",
    "            else:\n",
    "                infected_files += 1\n",
    "    \n",
    "    print(f\"Scanning complete. Directories scanned: {total_dirs}, Files scanned: {total_files}, Clean files: {clean_files}, Infected files: {infected_files}\")\n",
    "    return total_dirs, total_files, clean_files, infected_files\n",
    "\n",
    "local_dataset_path = \"/Users/shubhamagarwal/Documents/Northeastern/Big_Data_Assignment_2/gaia_dataset\"\n",
    "\n",
    "scan_local_dataset(local_dataset_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a6a10-22ff-4175-8404-45d8906478df",
   "metadata": {},
   "source": [
    "# Setup to use GCP Service account key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff7fc89-2124-4c05-aa2c-7761c48d5a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19c1bce0-1023-4439-8b87-c5c688d7ad98",
   "metadata": {},
   "source": [
    "# Function to upload scanned files to GCP Object bucket path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3eb1cd-22fe-4692-ab5c-f6875f2fd637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "# Function to upload files to GCP while preserving folder structure\n",
    "def upload_to_gcp(bucket_name, source_file_path, destination_blob_name, project_id):\n",
    "    \"\"\"Uploads a file to GCP bucket.\"\"\"\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Upload the file\n",
    "    blob.upload_from_filename(source_file_path)\n",
    "    print(f\"File {source_file_path} uploaded to {destination_blob_name}.\")\n",
    "\n",
    "# Function to upload all files to GCP, preserving folder structure\n",
    "def upload_all_files_to_gcp(local_dir, gcp_bucket_name, target_gcp_dir=\"\", project_id=None):\n",
    "\n",
    "    if project_id is None:\n",
    "        raise ValueError(\"Project ID is required to upload files to GCP.\")\n",
    "\n",
    "    print(f\"Uploading files from {local_dir} to GCP bucket {gcp_bucket_name}...\")\n",
    "\n",
    "    for root, dirs, files in os.walk(local_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            # Create the relative path from the base local directory\n",
    "            relative_path = os.path.relpath(file_path, local_dir)\n",
    "\n",
    "            # Add the target directory in GCP if provided\n",
    "            destination_blob_name = os.path.join(target_gcp_dir, relative_path) if target_gcp_dir else relative_path\n",
    "\n",
    "            # Upload the file\n",
    "            upload_to_gcp(gcp_bucket_name, file_path, destination_blob_name, project_id)\n",
    "\n",
    "    print(\"All files uploaded successfully.\")\n",
    "\n",
    "# Example usage\n",
    "GCP_BUCKET_NAME = \"bigdataia_fall2024_team9_assignment1_bucket\" \n",
    "LOCAL_DIR = \"/Users/shubhamagarwal/Documents/Northeastern/Big_Data_Assignment_2/gaia_dataset\" \n",
    "TARGET_GCP_DIR = \"project_2\"  \n",
    "PROJECT_ID = \"civil-tube-436417-k8\" \n",
    "\n",
    "upload_all_files_to_gcp(LOCAL_DIR, GCP_BUCKET_NAME, TARGET_GCP_DIR, project_id=PROJECT_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a6384-8da6-49d2-bbea-6bfe854d3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install google-cloud-documentai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4b380-188a-497a-945e-b74bd0a09e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade protobuf\n",
    "# ! pip install --upgrade google-cloud-storage\n",
    "# ! pip install --upgrade google-cloud-documentai\n",
    "# ! pip install --upgrade protobuf\n",
    "# ! pip install protobuf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4317c7c-174c-422f-923f-cfa54f45fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install protobuf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39815c48-7d9b-49de-b290-33ef18202acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdca1c5-d8a0-4b8c-b7df-d2cf9ef176c2",
   "metadata": {},
   "source": [
    "# Uploading Combined JSON Results to Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f673699-f10d-42da-9514-48204122afcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: project_2/2023/test/021a5339-744f-42b7-bd9b-9368b3efda7a.pdf\n",
      "Processing chunk: /tmp/split_1.pdf\n",
      "Uploaded combined result to project_2/gcp_document_api_processed/021a5339-744f-42b7-bd9b-9368b3efda7a_combined.json in bucket bigdataia_fall2024_team9_assignment1_bucket\n",
      "Processing file: project_2/2023/test/32f386b9-73ee-4455-b412-ddad508aa979.pdf\n",
      "Processing chunk: /tmp/split_1.pdf\n",
      "Uploaded combined result to project_2/gcp_document_api_processed/32f386b9-73ee-4455-b412-ddad508aa979_combined.json in bucket bigdataia_fall2024_team9_assignment1_bucket\n",
      "Processing file: project_2/2023/test/4044eab7-1282-42bd-a559-3bf3a4d5858e.pdf\n",
      "Processing chunk: /tmp/split_1.pdf\n",
      "Uploaded combined result to project_2/gcp_document_api_processed/4044eab7-1282-42bd-a559-3bf3a4d5858e_combined.json in bucket bigdataia_fall2024_team9_assignment1_bucket\n",
      "Processing file: project_2/2023/test/634fca59-03b2-4cdf-9ce4-0205df22f256.pdf\n",
      "Processing chunk: /tmp/split_1.pdf\n",
      "Uploaded combined result to project_2/gcp_document_api_processed/634fca59-03b2-4cdf-9ce4-0205df22f256_combined.json in bucket bigdataia_fall2024_team9_assignment1_bucket\n",
      "Processing file: project_2/2023/test/680d7d77-c0c7-49c8-88fd-f8ec623645e9.pdf\n",
      "Processing chunk: /tmp/split_1.pdf\n",
      "Uploaded combined result to project_2/gcp_document_api_processed/680d7d77-c0c7-49c8-88fd-f8ec623645e9_combined.json in bucket bigdataia_fall2024_team9_assignment1_bucket\n",
      "Processing file: project_2/2023/test/7c215d46-91c7-424e-9f22-37d43ab73ea6.pdf\n",
      "Processing chunk: /tmp/split_1.pdf\n",
      "Uploaded combined result to project_2/gcp_document_api_processed/7c215d46-91c7-424e-9f22-37d43ab73ea6_combined.json in bucket bigdataia_fall2024_team9_assignment1_bucket\n",
      "Processing file: project_2/2023/test/8f697523-6988-4c4f-8d72-760a45681f68.pdf\n",
      "Processing chunk: /tmp/split_1.pdf\n",
      "Processing chunk: /tmp/split_2.pdf\n",
      "Uploaded combined result to project_2/gcp_document_api_processed/8f697523-6988-4c4f-8d72-760a45681f68_combined.json in bucket bigdataia_fall2024_team9_assignment1_bucket\n",
      "Processing file: project_2/2023/test/b3654e47-4307-442c-a09c-945b33b913c6.pdf\n",
      "Processing chunk: /tmp/split_1.pdf\n",
      "Uploaded combined result to project_2/gcp_document_api_processed/b3654e47-4307-442c-a09c-945b33b913c6_combined.json in bucket bigdataia_fall2024_team9_assignment1_bucket\n",
      "Processing file: project_2/2023/test/be353748-74eb-4904-8f17-f180ce087f1a.pdf\n",
      "Processing chunk: /tmp/split_1.pdf\n",
      "Processing chunk: /tmp/split_2.pdf\n",
      "Processing chunk: /tmp/split_3.pdf\n",
      "Processing chunk: /tmp/split_4.pdf\n",
      "Processing chunk: /tmp/split_5.pdf\n",
      "Processing chunk: /tmp/split_6.pdf\n",
      "Processing chunk: /tmp/split_7.pdf\n",
      "Processing chunk: /tmp/split_8.pdf\n",
      "Processing chunk: /tmp/split_9.pdf\n",
      "Processing chunk: /tmp/split_10.pdf\n",
      "Processing chunk: /tmp/split_11.pdf\n",
      "Uploaded combined result to project_2/gcp_document_api_processed/be353748-74eb-4904-8f17-f180ce087f1a_combined.json in bucket bigdataia_fall2024_team9_assignment1_bucket\n",
      "Processing file: project_2/2023/test/c4456885-2f03-436f-8fe9-0b4ca6822cdb.pdf\n",
      "Processing chunk: /tmp/split_1.pdf\n",
      "Uploaded combined result to project_2/gcp_document_api_processed/c4456885-2f03-436f-8fe9-0b4ca6822cdb_combined.json in bucket bigdataia_fall2024_team9_assignment1_bucket\n",
      "Processing file: project_2/2023/test/ca0a4c14-4b97-43e7-8923-539d61050ae3.pdf\n",
      "Processing chunk: /tmp/split_1.pdf\n",
      "Uploaded combined result to project_2/gcp_document_api_processed/ca0a4c14-4b97-43e7-8923-539d61050ae3_combined.json in bucket bigdataia_fall2024_team9_assignment1_bucket\n",
      "Processing file: project_2/2023/test/d50b8ecb-a8aa-4696-ad84-403ef15e2c8b.pdf\n",
      "Processing chunk: /tmp/split_1.pdf\n",
      "Uploaded combined result to project_2/gcp_document_api_processed/d50b8ecb-a8aa-4696-ad84-403ef15e2c8b_combined.json in bucket bigdataia_fall2024_team9_assignment1_bucket\n",
      "Processing file: project_2/2023/validation/366e2f2b-8632-4ef2-81eb-bc3877489217.pdf\n",
      "Processing chunk: /tmp/split_1.pdf\n",
      "Uploaded combined result to project_2/gcp_document_api_processed/366e2f2b-8632-4ef2-81eb-bc3877489217_combined.json in bucket bigdataia_fall2024_team9_assignment1_bucket\n",
      "Processing file: project_2/2023/validation/67e8878b-5cef-4375-804e-e6291fdbe78a.pdf\n",
      "Processing chunk: /tmp/split_1.pdf\n",
      "Uploaded combined result to project_2/gcp_document_api_processed/67e8878b-5cef-4375-804e-e6291fdbe78a_combined.json in bucket bigdataia_fall2024_team9_assignment1_bucket\n",
      "Processing file: project_2/2023/validation/e9a2c537-8232-4c3f-85b0-b52de6bcba99.pdf\n",
      "Processing chunk: /tmp/split_1.pdf\n",
      "Uploaded combined result to project_2/gcp_document_api_processed/e9a2c537-8232-4c3f-85b0-b52de6bcba99_combined.json in bucket bigdataia_fall2024_team9_assignment1_bucket\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import storage, documentai_v1 as documentai\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "# Initialize the Document AI client\n",
    "def get_document_ai_client():\n",
    "    return documentai.DocumentProcessorServiceClient()\n",
    "\n",
    "# Process a PDF file using Document AI\n",
    "def process_document(project_id, location, processor_id, file_path):\n",
    "    client = get_document_ai_client()\n",
    "\n",
    "    # Read the PDF from local storage\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        content = pdf_file.read()\n",
    "\n",
    "    # Configure the request for Document AI API\n",
    "    raw_document = {\"content\": content, \"mime_type\": \"application/pdf\"}\n",
    "    name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "\n",
    "    # Call the API to process the document\n",
    "    request = documentai.ProcessRequest(name=name, raw_document=raw_document)\n",
    "    result = client.process_document(request=request)\n",
    "    return result.document\n",
    "\n",
    "# Upload combined JSON results to GCP bucket\n",
    "def upload_json_to_gcp(bucket_name, destination_blob_name, content):\n",
    "    storage_client = storage.Client(project=\"civil-tube-436417-k8\")\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Upload the JSON content\n",
    "    blob.upload_from_string(content)\n",
    "    print(f\"Uploaded combined result to {destination_blob_name} in bucket {bucket_name}\")\n",
    "\n",
    "# Split the PDF into chunks of a maximum of 15 pages\n",
    "def split_pdf(input_pdf_path, output_dir, chunk_size=15):\n",
    "    pdf_reader = PdfReader(input_pdf_path)\n",
    "    total_pages = len(pdf_reader.pages)\n",
    "    chunk_paths = []\n",
    "\n",
    "    for i in range(0, total_pages, chunk_size):\n",
    "        pdf_writer = PdfWriter()\n",
    "        chunk_filename = os.path.join(output_dir, f\"split_{i // chunk_size + 1}.pdf\")\n",
    "\n",
    "        for j in range(i, min(i + chunk_size, total_pages)):\n",
    "            pdf_writer.add_page(pdf_reader.pages[j])\n",
    "\n",
    "        with open(chunk_filename, 'wb') as output_pdf:\n",
    "            pdf_writer.write(output_pdf)\n",
    "        \n",
    "        chunk_paths.append(chunk_filename)\n",
    "\n",
    "    return chunk_paths\n",
    "\n",
    "# Process each chunked PDF, combine the results, and upload the combined result\n",
    "def process_and_upload_combined_chunks(project_id, location, processor_id, bucket_name, pdf_file, output_dir):\n",
    "    # Split the large PDF into chunks\n",
    "    chunk_paths = split_pdf(pdf_file, \"/tmp\")\n",
    "\n",
    "    combined_text = \"\"\n",
    "    for chunk_path in chunk_paths:\n",
    "        print(f\"Processing chunk: {chunk_path}\")\n",
    "        processed_doc = process_document(project_id, location, processor_id, chunk_path)\n",
    "\n",
    "        # Combine the text from each processed chunk\n",
    "        combined_text += processed_doc.text\n",
    "\n",
    "    # Upload the combined result as one JSON file\n",
    "    json_output_path = f\"{output_dir}/{os.path.basename(pdf_file).replace('.pdf', '_combined.json')}\"\n",
    "    full_output_path = os.path.join(\"project_2\", json_output_path)  \n",
    "    upload_json_to_gcp(bucket_name, full_output_path, combined_text)\n",
    "\n",
    "# Function to list PDF files in nested directories of the GCP bucket\n",
    "def list_pdfs_in_bucket(bucket_name, prefix):\n",
    "    storage_client = storage.Client(project=\"civil-tube-436417-k8\")\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Recursively list all blobs in the bucket with the specified prefix (directory)\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "    pdf_files = [blob.name for blob in blobs if blob.name.endswith(\".pdf\")]\n",
    "    return pdf_files\n",
    "\n",
    "# Main function to process PDFs from GCP bucket, combine results, and upload the combined result\n",
    "def process_pdfs_in_bucket(bucket_name, prefix, project_id, location, processor_id, output_dir):\n",
    "    # List all PDF files in the GCP bucket\n",
    "    pdf_files = list_pdfs_in_bucket(bucket_name, prefix)\n",
    "\n",
    "    # Process each PDF and combine results\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing file: {pdf_file}\")\n",
    "\n",
    "        # Download the file to local temporarily\n",
    "        local_pdf_path = f\"/tmp/{os.path.basename(pdf_file)}\"\n",
    "        storage_client = storage.Client(project=\"civil-tube-436417-k8\")\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(pdf_file)\n",
    "        blob.download_to_filename(local_pdf_path)\n",
    "\n",
    "        # Process the PDF using Document AI API in chunks and combine results\n",
    "        process_and_upload_combined_chunks(project_id, location, processor_id, bucket_name, local_pdf_path, output_dir)\n",
    "\n",
    "# Parameters\n",
    "# PROJECT_ID = \"civil-tube-436417-k8\"  \n",
    "# LOCATION = \"us\" \n",
    "# PROCESSOR_ID = \"d32ef0b7fb707d\"  \n",
    "# BUCKET_NAME = \"bigdataia_fall2024_team9_assignment1_bucket\" \n",
    "# PREFIX = \"project_2\" \n",
    "# OUTPUT_DIR = \"gcp_document_api_processed\"  \n",
    "\n",
    "# Call the function to process and store the parsed results\n",
    "process_pdfs_in_bucket(BUCKET_NAME, PREFIX, PROJECT_ID, LOCATION, PROCESSOR_ID, OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be2169-d2ff-4c3a-9533-4bbba120f2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617b583b-e5b3-48db-8bdc-bc024c5928f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4dad3d-bc2c-40c5-b513-2c1585590ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1992b4fc-c89f-4bb2-9dc3-4db916183c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
